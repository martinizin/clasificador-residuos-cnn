# â™»ï¸ Clasificador de Residuos con CNN y Streamlit

> **Proyecto de Inteligencia Artificial I** â€” ClasificaciÃ³n automÃ¡tica de imÃ¡genes de residuos en 6 categorÃ­as utilizando Transfer Learning con MobileNetV2 y despliegue interactivo con Streamlit.

---

## ðŸ“‹ Tabla de Contenidos

1. [Resumen Ejecutivo](#resumen-ejecutivo)
2. [Arquitectura de la SoluciÃ³n](#arquitectura-de-la-soluciÃ³n)
3. [Requisitos e InstalaciÃ³n](#requisitos-e-instalaciÃ³n)
4. [PreparaciÃ³n del Dataset](#preparaciÃ³n-del-dataset)
5. [Entrenamiento del Modelo](#entrenamiento-del-modelo)
6. [Fundamentos TeÃ³ricos](#fundamentos-teÃ³ricos)
7. [ExplicaciÃ³n Detallada del CÃ³digo](#explicaciÃ³n-detallada-del-cÃ³digo)
8. [EjecuciÃ³n Local](#ejecuciÃ³n-local)
9. [Despliegue en Streamlit Cloud](#despliegue-en-streamlit-cloud)

---

## Resumen Ejecutivo

### Â¿QuÃ© hace este proyecto?

Este proyecto implementa un **sistema de clasificaciÃ³n automÃ¡tica de residuos** basado en imÃ¡genes. Dado una fotografÃ­a de un objeto (cartÃ³n, vidrio, metal, papel, plÃ¡stico o basura genÃ©rica), el modelo predice a cuÃ¡l de las **6 categorÃ­as** pertenece.

### Â¿A quiÃ©n sirve?

- **Estudiantes** aprendiendo sobre redes neuronales convolucionales (CNN) y Transfer Learning.
- **Desarrolladores** que desean un ejemplo prÃ¡ctico de ML end-to-end (entrenamiento + despliegue).
- **Proyectos ambientales** que necesiten automatizar la clasificaciÃ³n de residuos.

### Clases que clasifica el modelo

| Clase       | DescripciÃ³n                        |
|-------------|------------------------------------|
| `cardboard` | CartÃ³n (cajas, empaques)           |
| `glass`     | Vidrio (botellas, frascos)         |
| `metal`     | Metal (latas, aluminio)            |
| `paper`     | Papel (hojas, periÃ³dicos)          |
| `plastic`   | PlÃ¡stico (botellas, envases)       |
| `trash`     | Basura genÃ©rica (no reciclable)    |

### Demo

Al ejecutar la aplicaciÃ³n, verÃ¡s una interfaz como esta:

```
â™»ï¸ Clasificador de Residuos (6 clases)
Sube una imagen y el modelo predice: cardboard, glass, metal, paper, plastic, trash.

[ðŸ“ Sube una imagen (JPG/PNG)]

PredicciÃ³n final: plastic â€” probabilidad: 0.9234
Top 3:
- plastic: 0.9234
- glass: 0.0521
- metal: 0.0189

[GrÃ¡fico de barras con todas las probabilidades]
```

---

## Arquitectura de la SoluciÃ³n

El proyecto sigue un **pipeline clÃ¡sico de Machine Learning**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              PIPELINE COMPLETO                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                      â”‚
â”‚   ðŸ“ Dataset        ðŸ”„ Preprocesamiento      ðŸ§  Entrenamiento      ðŸ“Š EvaluaciÃ³n    â”‚
â”‚   (Kaggle)    â”€â”€â”€â–º  (Resize, Split,    â”€â”€â”€â–º  (MobileNetV2 +   â”€â”€â”€â–º (Accuracy,       â”‚
â”‚   6 carpetas        Augmentation)            Transfer Learning)    Val Loss)        â”‚
â”‚                                                                                      â”‚
â”‚        â”‚                                                                             â”‚
â”‚        â–¼                                                                             â”‚
â”‚                                                                                      â”‚
â”‚   ðŸ’¾ ExportaciÃ³n       ðŸ”® Inferencia         ðŸŒ Streamlit         â˜ï¸ Deploy         â”‚
â”‚   (model.keras,   â—„â”€â”€â”€ (PredicciÃ³n      â—„â”€â”€â”€ (Interfaz web)  â—„â”€â”€â”€ (Streamlit        â”‚
â”‚    labels.json)        con imagen)                                  Cloud)          â”‚
â”‚                                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Diagrama de archivos generados

```
Entrenamiento (train.py)              Inferencia (app.py)
         â”‚                                   â”‚
         â–¼                                   â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ model.keras â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  Cargar     â”‚
   â”‚ labels.json â”‚                    â”‚  modelo     â”‚
   â”‚ meta.json   â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
                                             â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ PredicciÃ³n      â”‚
                                    â”‚ + Top-3         â”‚
                                    â”‚ + GrÃ¡fico       â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Requisitos e InstalaciÃ³n

### Prerrequisitos

| Requisito      | VersiÃ³n mÃ­nima | Verificar con           |
|----------------|----------------|-------------------------|
| Python         | 3.9+           | `python --version`      |
| pip            | 21.0+          | `pip --version`         |
| Git (opcional) | 2.0+           | `git --version`         |

### Estructura de carpetas del proyecto

```
modeloIA/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ cardboard/    # ~400 imÃ¡genes
â”‚       â”œâ”€â”€ glass/        # ~500 imÃ¡genes
â”‚       â”œâ”€â”€ metal/        # ~400 imÃ¡genes
â”‚       â”œâ”€â”€ paper/        # ~590 imÃ¡genes
â”‚       â”œâ”€â”€ plastic/      # ~480 imÃ¡genes
â”‚       â””â”€â”€ trash/        # ~130 imÃ¡genes
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ model.keras       # Modelo entrenado
â”‚   â”œâ”€â”€ labels.json       # ["cardboard", "glass", ...]
â”‚   â””â”€â”€ meta.json         # {"img_size": 224, "arch": "MobileNetV2"}
â”œâ”€â”€ train.py              # Script de entrenamiento
â”œâ”€â”€ app.py                # AplicaciÃ³n Streamlit
â”œâ”€â”€ requirements.txt      # Dependencias
â””â”€â”€ README.md             # Este archivo
```

### Paso 1: Clonar o descargar el repositorio

```bash
# Si tienes Git:
git clone https://github.com/tu-usuario/recycle-cnn.git
cd recycle-cnn

# Si descargaste ZIP: descomprimir y abrir carpeta en terminal
```

### Paso 2: Crear entorno virtual

**Windows (PowerShell):**
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

> âš ï¸ **Si da error de polÃ­ticas de ejecuciÃ³n**, ejecuta primero:
> ```powershell
> Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
> ```

**Windows (CMD):**
```cmd
python -m venv venv
venv\Scripts\activate.bat
```

**Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### Paso 3: Instalar dependencias

```bash
pip install -r requirements.txt
```

Contenido de `requirements.txt`:
```
streamlit
tensorflow
pillow
numpy
```

### Paso 4: Verificar instalaciÃ³n

```bash
python -c "import tensorflow as tf; print(f'TensorFlow: {tf.__version__}')"
python -c "import streamlit; print(f'Streamlit: {streamlit.__version__}')"
```

---

## PreparaciÃ³n del Dataset

### Fuente del dataset

El dataset proviene de Kaggle: **[Garbage Classification (6 classes)](https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification)**.

Contiene aproximadamente **2,500 imÃ¡genes** distribuidas en 6 clases.

### Descarga manual

1. Ir a Kaggle y descargar el dataset (requiere cuenta gratuita).
2. Descomprimir el archivo ZIP.
3. Organizar las imÃ¡genes en la estructura `data/raw/`:

```
data/
â””â”€â”€ raw/
    â”œâ”€â”€ cardboard/
    â”‚   â”œâ”€â”€ cardboard1.jpg
    â”‚   â”œâ”€â”€ cardboard2.jpg
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ glass/
    â”‚   â”œâ”€â”€ glass1.jpg
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ metal/
    â”œâ”€â”€ paper/
    â”œâ”€â”€ plastic/
    â””â”€â”€ trash/
```

### Validar estructura

Ejecuta este comando para verificar que las carpetas existen:

**Windows:**
```powershell
Get-ChildItem data\raw -Directory | ForEach-Object { 
    Write-Host "$($_.Name): $((Get-ChildItem $_.FullName -File).Count) imÃ¡genes" 
}
```

**Linux/Mac:**
```bash
for dir in data/raw/*/; do echo "$(basename $dir): $(ls -1 $dir | wc -l) imÃ¡genes"; done
```

Salida esperada (aproximada):
```
cardboard: 403 imÃ¡genes
glass: 501 imÃ¡genes
metal: 410 imÃ¡genes
paper: 594 imÃ¡genes
plastic: 482 imÃ¡genes
trash: 137 imÃ¡genes
```

---

## Entrenamiento del Modelo

### Comando bÃ¡sico

```bash
python train.py
```

Esto usa los valores por defecto:
- `--data_dir`: `data/raw`
- `--img_size`: `224`
- `--batch`: `32`
- `--epochs`: `5`
- `--fine_tune_epochs`: `3`

### Comando con parÃ¡metros personalizados

```bash
python train.py --data_dir data/raw --img_size 224 --batch 32 --epochs 10 --fine_tune_epochs 5
```

### HiperparÃ¡metros explicados

| ParÃ¡metro          | Valor default | DescripciÃ³n                                                                 |
|--------------------|---------------|-----------------------------------------------------------------------------|
| `--data_dir`       | `data/raw`    | Carpeta que contiene las subcarpetas de clases                              |
| `--img_size`       | `224`         | TamaÃ±o al que se redimensionan las imÃ¡genes (224Ã—224 pÃ­xeles)               |
| `--batch`          | `32`          | NÃºmero de imÃ¡genes procesadas en paralelo por iteraciÃ³n                     |
| `--epochs`         | `5`           | Ã‰pocas de entrenamiento con la base congelada (solo cabeza)                 |
| `--fine_tune_epochs`| `3`          | Ã‰pocas adicionales con la base descongelada (fine-tuning)                   |

### Archivos generados

DespuÃ©s del entrenamiento, la carpeta `model/` contendrÃ¡:

| Archivo         | Contenido                                                    |
|-----------------|--------------------------------------------------------------|
| `model.keras`   | Modelo completo (arquitectura + pesos) en formato Keras 3    |
| `labels.json`   | Lista ordenada de clases: `["cardboard", "glass", ...]`      |
| `meta.json`     | Metadatos: `{"img_size": 224, "arch": "MobileNetV2"}`        |

**Â¿Por quÃ© guardar `labels.json` y `meta.json`?**

Para asegurar **consistencia** entre entrenamiento e inferencia:
- El orden de las clases puede variar si se re-entrena en otra mÃ¡quina.
- El tamaÃ±o de imagen debe coincidir exactamente.

---

## Fundamentos TeÃ³ricos

Esta secciÃ³n explica los conceptos clave para **defender el proyecto** ante un jurado.

### Â¿QuÃ© es una CNN (Red Neuronal Convolucional)?

Una **CNN** es un tipo de red neuronal diseÃ±ada especÃ­ficamente para procesar datos con estructura de grilla, como imÃ¡genes.

**Â¿Por quÃ© CNN para imÃ¡genes?**

1. **ExtracciÃ³n jerÃ¡rquica de caracterÃ­sticas**: Las primeras capas detectan bordes y texturas; las profundas detectan formas y objetos.
2. **Invariancia espacial**: Puede detectar un objeto sin importar dÃ³nde estÃ© en la imagen.
3. **ReducciÃ³n de parÃ¡metros**: Usa convoluciones en lugar de conexiones densas, reduciendo la memoria necesaria.

```
Imagen (224Ã—224Ã—3) â†’ Convoluciones â†’ Pooling â†’ ... â†’ Features â†’ Dense â†’ Softmax â†’ Clase
```

### Â¿QuÃ© es Transfer Learning?

**Transfer Learning** es la tÃ©cnica de reutilizar un modelo entrenado en un problema similar para resolver uno nuevo.

**AnalogÃ­a**: Es como si un chef experto en cocina italiana aprendiera cocina japonesa. No parte de cero; ya sabe tÃ©cnicas de corte, tiempos de cocciÃ³n, etc.

**Â¿Por quÃ© usarlo?**
- Nuestro dataset es pequeÃ±o (~2,500 imÃ¡genes).
- Entrenar una CNN desde cero requerirÃ­a millones de imÃ¡genes.
- MobileNetV2 ya "sabe" detectar texturas, formas y objetos genÃ©ricos.

### Â¿Por quÃ© MobileNetV2?

| CaracterÃ­stica        | MobileNetV2              | VGG16           | ResNet50       |
|-----------------------|--------------------------|-----------------|----------------|
| ParÃ¡metros            | ~3.4M                    | ~138M           | ~25.6M         |
| TamaÃ±o del archivo    | ~14 MB                   | ~528 MB         | ~98 MB         |
| Velocidad en CPU      | RÃ¡pida                   | Lenta           | Media          |
| PrecisiÃ³n en ImageNet | 71.8%                    | 71.3%           | 74.9%          |

**Ventajas de MobileNetV2:**
- Ligero: ideal para despliegue en la nube (lÃ­mites de Streamlit Cloud).
- RÃ¡pido: buena experiencia de usuario.
- Eficiente: usa "depthwise separable convolutions" que reducen cÃ³mputo.

### Â¿QuÃ© es ImageNet?

**ImageNet** es un dataset de ~14 millones de imÃ¡genes etiquetadas en ~22,000 categorÃ­as. El subconjunto ILSVRC tiene 1,000 clases (animales, objetos, vehÃ­culos, etc.).

Al usar `weights="imagenet"`, cargamos pesos de MobileNetV2 entrenada en este dataset. Estos pesos codifican conocimiento visual general que transferimos a nuestro problema.

### Â¿QuÃ© es Softmax?

**Softmax** es una funciÃ³n de activaciÃ³n que convierte un vector de valores reales en una distribuciÃ³n de probabilidad.

```
Entradas (logits):  [2.0, 1.0, 0.1, 0.5, 3.0, 0.8]
Salidas (probs):    [0.10, 0.04, 0.01, 0.02, 0.79, 0.03]
                                 â”‚
                                 â””â”€â”€ Suman 1.0
```

Se usa en clasificaciÃ³n multiclase porque:
- Las probabilidades son interpretables (79% confianza en "plastic").
- Permite calcular el top-3 de predicciones.

### Â¿QuÃ© es Sparse Categorical Crossentropy?

Es la **funciÃ³n de pÃ©rdida** que mide quÃ© tan "equivocado" estÃ¡ el modelo.

**Â¿Por quÃ© "sparse"?**
- Nuestras etiquetas son enteros: `0, 1, 2, 3, 4, 5`.
- Si fueran one-hot (`[0,0,1,0,0,0]`), usarÃ­amos `categorical_crossentropy`.
- "Sparse" evita convertir a one-hot, ahorrando memoria.

**FÃ³rmula simplificada:**
```
Loss = -log(probabilidad de la clase correcta)
```

Si el modelo predice 0.9 para la clase correcta: `loss = -log(0.9) â‰ˆ 0.10` (bajo).
Si predice 0.1: `loss = -log(0.1) â‰ˆ 2.30` (alto).

### Â¿QuÃ© es Overfitting y cÃ³mo lo evitamos?

**Overfitting** ocurre cuando el modelo memoriza los datos de entrenamiento pero no generaliza a datos nuevos.

**SeÃ±ales de overfitting:**
- Accuracy de entrenamiento: 99%
- Accuracy de validaciÃ³n: 60%

**TÃ©cnicas de mitigaciÃ³n usadas en este proyecto:**

| TÃ©cnica          | DÃ³nde se aplica           | Efecto                                        |
|------------------|---------------------------|-----------------------------------------------|
| Data Augmentation| `RandomFlip`, `RandomRotation`, `RandomZoom` | Genera variaciones artificiales de imÃ¡genes |
| Dropout          | `Dropout(0.2)`            | Apaga 20% de neuronas aleatoriamente          |
| Transfer Learning| Base congelada inicialmente| Aprovecha features pre-aprendidos            |
| Fine-tuning      | Learning rate muy bajo (1e-5)| Ajusta pesos sin destruir conocimiento      |

### Epochs, Batch Size y Learning Rate

| Concepto      | DefiniciÃ³n                                      | Valor usado    |
|---------------|-------------------------------------------------|----------------|
| **Epoch**     | Una pasada completa por todo el dataset          | 5 + 3          |
| **Batch Size**| NÃºmero de imÃ¡genes procesadas antes de actualizar pesos | 32       |
| **Learning Rate** | QuÃ© tan grande es el "paso" al ajustar pesos | 1e-3 â†’ 1e-5   |

**AnalogÃ­a del learning rate:**
- Muy alto (0.1): Caminas dando saltos enormes; puedes pasar de largo el mÃ­nimo.
- Muy bajo (1e-7): Caminas milÃ­metro a milÃ­metro; nunca llegas.
- Justo (1e-3 a 1e-5): Pasos razonables hacia el objetivo.

### MÃ©tricas: Accuracy

**Accuracy** = (predicciones correctas) / (total de predicciones) Ã— 100

Ejemplo: Si de 100 imÃ¡genes, 85 se clasifican bien â†’ Accuracy = 85%.

**LimitaciÃ³n**: Si el dataset estÃ¡ desbalanceado (trash tiene pocas imÃ¡genes), accuracy puede engaÃ±ar.

**Mejoras sugeridas:**
- **Confusion Matrix**: Muestra errores por clase.
- **Precision/Recall/F1**: MÃ©tricas mÃ¡s robustas para clases desbalanceadas.

---

## ExplicaciÃ³n Detallada del CÃ³digo

### ðŸ“„ train.py â€” Bloque por Bloque

#### Bloque 1: Imports y configuraciÃ³n

```python
# train.py
import argparse          # Para leer argumentos de lÃ­nea de comandos
import json              # Para guardar labels.json y meta.json
import os                # Para crear carpetas (os.makedirs)
import tensorflow as tf  # Framework de deep learning
from tensorflow import keras  # API de alto nivel para redes neuronales
```

#### Bloque 2: FunciÃ³n principal con parÃ¡metros

```python
def main(data_dir: str, img_size: int, batch: int, epochs: int, fine_tune_epochs: int):
    img_shape = (img_size, img_size)  # Tupla (224, 224) para redimensionar imÃ¡genes
```
- `data_dir`: Carpeta con las subcarpetas de clases.
- `img_size`: TamaÃ±o de imagen (MobileNetV2 espera 224Ã—224).
- `batch`: ImÃ¡genes por lote.
- `epochs`: Ã‰pocas con base congelada.
- `fine_tune_epochs`: Ã‰pocas con base descongelada.

#### Bloque 3: Carga del dataset

```python
    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_dir,                    # Ruta: data/raw/
        validation_split=0.2,        # 20% para validaciÃ³n
        subset="training",           # Este es el 80% de entrenamiento
        seed=1337,                   # Semilla para reproducibilidad
        image_size=img_shape,        # Redimensiona a (224, 224)
        batch_size=batch,            # 32 imÃ¡genes por lote
        label_mode="int",            # Labels como enteros: 0, 1, 2...
    )
```

**Â¿QuÃ© hace `image_dataset_from_directory`?**
1. Lee la estructura de carpetas.
2. Asigna un nÃºmero a cada subcarpeta (en orden alfabÃ©tico):
   - `cardboard` â†’ 0
   - `glass` â†’ 1
   - `metal` â†’ 2
   - `paper` â†’ 3
   - `plastic` â†’ 4
   - `trash` â†’ 5
3. Redimensiona cada imagen a 224Ã—224.
4. Agrupa en batches de 32.

```python
    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="validation",         # Este es el 20% de validaciÃ³n
        seed=1337,                   # Â¡Misma semilla! Para que no se mezclen
        image_size=img_shape,
        batch_size=batch,
        label_mode="int",
    )
```

#### Bloque 4: Guardar metadatos

```python
    class_names = train_ds.class_names  # ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']
    os.makedirs("model", exist_ok=True)  # Crea carpeta model/ si no existe

    with open("model/labels.json", "w", encoding="utf-8") as f:
        json.dump(class_names, f, ensure_ascii=False, indent=2)
        # Guarda: ["cardboard", "glass", "metal", "paper", "plastic", "trash"]

    with open("model/meta.json", "w", encoding="utf-8") as f:
        json.dump({"img_size": img_size, "arch": "MobileNetV2"}, f, indent=2)
        # Guarda: {"img_size": 224, "arch": "MobileNetV2"}
```

**Â¿Por quÃ© guardar esto?**
- `labels.json`: Para que `app.py` sepa quÃ© significa cada Ã­ndice.
- `meta.json`: Para que `app.py` redimensione igual que en entrenamiento.

#### Bloque 5: Pipeline de datos optimizado

```python
    AUTOTUNE = tf.data.AUTOTUNE  # TensorFlow decide automÃ¡ticamente el paralelismo
    train_ds = train_ds.cache().shuffle(1000).prefetch(AUTOTUNE)
    val_ds = val_ds.cache().prefetch(AUTOTUNE)
```

| MÃ©todo     | QuÃ© hace                                                      |
|------------|---------------------------------------------------------------|
| `cache()`  | Guarda en memoria los datos tras la primera lectura           |
| `shuffle(1000)` | Mezcla 1000 elementos para que el modelo no vea patrones de orden |
| `prefetch(AUTOTUNE)` | Prepara el siguiente batch mientras la GPU entrena el actual |

#### Bloque 6: Data Augmentation

```python
    data_augmentation = keras.Sequential(
        [
            keras.layers.RandomFlip("horizontal"),  # Voltea horizontalmente (50% prob)
            keras.layers.RandomRotation(0.05),      # Rota hasta Â±18Â° (0.05 * 360Â°)
            keras.layers.RandomZoom(0.1),           # Zoom hasta Â±10%
        ]
    )
```

**Â¿Por quÃ© augmentation?**
- El dataset es pequeÃ±o (~2,500 imÃ¡genes).
- Augmentation genera variaciones artificiales.
- Reduce overfitting: el modelo no memoriza imÃ¡genes exactas.

#### Bloque 7: Modelo base (MobileNetV2)

```python
    base = keras.applications.MobileNetV2(
        input_shape=img_shape + (3,),  # (224, 224, 3) â€” 3 canales RGB
        include_top=False,              # Sin la capa de clasificaciÃ³n original (1000 clases de ImageNet)
        weights="imagenet",             # Pesos preentrenados en ImageNet
    )
    base.trainable = False              # Â¡Congelamos! No se actualizan estos pesos (todavÃ­a)
```

**Â¿QuÃ© significa `include_top=False`?**
- MobileNetV2 original termina en una capa Dense de 1000 neuronas (clases de ImageNet).
- Nosotros solo tenemos 6 clases; no nos sirve esa capa.
- `include_top=False` nos da solo el "extractor de caracterÃ­sticas".

#### Bloque 8: ConstrucciÃ³n del modelo completo

```python
    inputs = keras.Input(shape=img_shape + (3,))   # Entrada: (224, 224, 3)
    x = data_augmentation(inputs)                   # Aplica augmentation
    x = keras.applications.mobilenet_v2.preprocess_input(x)  # Normaliza a [-1, 1]
    x = base(x, training=False)                     # Pasa por MobileNetV2 (sin entrenar)
    x = keras.layers.GlobalAveragePooling2D()(x)   # Reduce (7, 7, 1280) â†’ (1280,)
    x = keras.layers.Dropout(0.2)(x)               # Apaga 20% de neuronas (regularizaciÃ³n)
    outputs = keras.layers.Dense(len(class_names), activation="softmax")(x)  # 6 neuronas
    model = keras.Model(inputs, outputs)
```

**Flujo de datos:**
```
Imagen (224,224,3)
    â†“ Augmentation
Imagen aumentada (224,224,3)
    â†“ preprocess_input (normaliza pÃ­xeles de [0,255] a [-1,1])
Tensor normalizado (224,224,3)
    â†“ MobileNetV2 (base congelada)
Features (7,7,1280)
    â†“ GlobalAveragePooling2D
Vector (1280,)
    â†“ Dropout(0.2)
Vector (1280,) con algunas neuronas "apagadas"
    â†“ Dense(6, softmax)
Probabilidades (6,) â†’ [0.1, 0.05, 0.02, 0.03, 0.78, 0.02]
```

#### Bloque 9: CompilaciÃ³n y entrenamiento (fase 1)

```python
    model.compile(
        optimizer=keras.optimizers.Adam(1e-3),      # Learning rate = 0.001
        loss="sparse_categorical_crossentropy",      # PÃ©rdida para clasificaciÃ³n
        metrics=["accuracy"],                        # MÃ©trica a monitorear
    )

    print("\n== Entrenamiento (cabeza) ==")
    model.fit(train_ds, validation_data=val_ds, epochs=epochs)  # epochs=5
```

**Â¿Por quÃ© `Adam`?**
- Adaptativo: ajusta el learning rate por parÃ¡metro.
- Robusto: funciona bien sin mucho tuning.

#### Bloque 10: Fine-tuning (fase 2)

```python
    if fine_tune_epochs > 0:
        print("\n== Fine-tuning (descongelar base) ==")
        base.trainable = True  # Â¡Ahora sÃ­ se actualizan los pesos de MobileNetV2!
        model.compile(
            optimizer=keras.optimizers.Adam(1e-5),  # Learning rate MUCHO mÃ¡s bajo
            loss="sparse_categorical_crossentropy",
            metrics=["accuracy"],
        )
        model.fit(train_ds, validation_data=val_ds, epochs=fine_tune_epochs)  # epochs=3
```

**Â¿Por quÃ© learning rate 1e-5 en fine-tuning?**
- Los pesos de MobileNetV2 ya estÃ¡n "bien".
- Un learning rate alto (1e-3) los destruirÃ­a.
- 1e-5 = ajustes finos, sutiles.

#### Bloque 11: Guardar modelo

```python
    model.save("model/model.keras")
    print("\n Modelo guardado en: model/model.keras")
```

**Â¿Por quÃ© `.keras` y no `.h5`?**
- `.keras` es el formato nativo de Keras 3 (TensorFlow 2.16+).
- Guarda arquitectura + pesos + configuraciÃ³n del optimizador.
- MÃ¡s robusto que `.h5` para modelos con capas personalizadas.

#### Bloque 12: CLI (Command Line Interface)

```python
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=str, default="data/raw")
    parser.add_argument("--img_size", type=int, default=224)
    parser.add_argument("--batch", type=int, default=32)
    parser.add_argument("--epochs", type=int, default=5)
    parser.add_argument("--fine_tune_epochs", type=int, default=3)
    args = parser.parse_args()

    main(args.data_dir, args.img_size, args.batch, args.epochs, args.fine_tune_epochs)
```

Esto permite ejecutar:
```bash
python train.py --epochs 10 --batch 64
```

---

### ðŸ“„ app.py â€” Bloque por Bloque

#### Bloque 1: Imports

```python
# app.py
import json              # Para leer labels.json y meta.json
import numpy as np       # Para manipular arrays
import streamlit as st   # Framework de la interfaz web
from PIL import Image    # Para abrir y redimensionar imÃ¡genes
import tensorflow as tf  # Para cargar y ejecutar el modelo
```

#### Bloque 2: ConfiguraciÃ³n de pÃ¡gina

```python
st.set_page_config(page_title="Recycle CNN", layout="centered")
```

- `page_title`: TÃ­tulo en la pestaÃ±a del navegador.
- `layout="centered"`: Contenido centrado (vs. "wide" que usa todo el ancho).

#### Bloque 3: Carga de artefactos (con cachÃ©)

```python
@st.cache_resource
def load_artifacts():
    model = tf.keras.models.load_model("model/model.keras")
    with open("model/labels.json", "r", encoding="utf-8") as f:
        labels = json.load(f)
    with open("model/meta.json", "r", encoding="utf-8") as f:
        meta = json.load(f)
    return model, labels, meta
```

**Â¿QuÃ© hace `@st.cache_resource`?**
- Streamlit recarga el script en cada interacciÃ³n del usuario.
- Sin cachÃ©, cargarÃ­a el modelo (14MB) cada vez â†’ muy lento.
- Con cachÃ©, carga una vez y reutiliza en memoria.

```python
model, labels, meta = load_artifacts()
IMG_SIZE = int(meta.get("img_size", 224))
```

- `meta.get("img_size", 224)`: Si no existe la clave, usa 224 por defecto.

#### Bloque 4: Interfaz de usuario

```python
st.title("â™»ï¸ Clasificador de Residuos (6 clases)")
st.write("Sube una imagen y el modelo predice: cardboard, glass, metal, paper, plastic, trash.")

uploaded = st.file_uploader("Sube una imagen (JPG/PNG)", type=["jpg", "jpeg", "png"])
```

- `st.title()`: Encabezado grande.
- `st.write()`: Texto normal.
- `st.file_uploader()`: Widget para subir archivos. Retorna `None` si no hay archivo.

#### Bloque 5: Procesamiento de imagen

```python
if uploaded:
    img = Image.open(uploaded).convert("RGB")  # Abre y convierte a RGB (sin canal alpha)
    st.image(img, caption="Imagen subida", use_container_width=True)  # Muestra preview
```

**Â¿Por quÃ© `.convert("RGB")`?**
- Algunas imÃ¡genes PNG tienen 4 canales (RGBA).
- El modelo espera 3 canales (RGB).

```python
    img_resized = img.resize((IMG_SIZE, IMG_SIZE))  # Redimensiona a 224Ã—224
    x = np.array(img_resized, dtype=np.float32)     # Convierte a array numpy
    x = np.expand_dims(x, axis=0)                   # AÃ±ade dimensiÃ³n batch: (1, 224, 224, 3)
```

**Â¿Por quÃ© `expand_dims`?**
- El modelo espera entrada con forma `(batch, height, width, channels)`.
- Una imagen tiene `(height, width, channels)`.
- `expand_dims` aÃ±ade la dimensiÃ³n de batch: `(1, 224, 224, 3)`.

#### Bloque 6: PredicciÃ³n

```python
    preds = model.predict(x, verbose=0)[0]  # PredicciÃ³n â†’ (6,) probabilidades
    top = int(np.argmax(preds))             # Ãndice de la probabilidad mÃ¡s alta
```

- `model.predict(x)` retorna `(1, 6)` (batch de 1, 6 clases).
- `[0]` extrae el primer (y Ãºnico) elemento: `(6,)`.
- `np.argmax(preds)` retorna el Ã­ndice del valor mÃ¡ximo.

#### Bloque 7: Mostrar resultados

```python
    st.subheader("PredicciÃ³n final")
    st.write(f"**{labels[top]}**  â€”  probabilidad: **{preds[top]:.4f}**")
```

Ejemplo de salida: **plastic** â€” probabilidad: **0.9234**

```python
    st.subheader("Top 3")
    top3 = sorted(list(enumerate(preds)), key=lambda t: t[1], reverse=True)[:3]
    for i, p in top3:
        st.write(f"- {labels[i]}: {p:.4f}")
```

**Desglose:**
1. `enumerate(preds)` â†’ `[(0, 0.10), (1, 0.04), ..., (4, 0.79), (5, 0.03)]`
2. `sorted(..., reverse=True)` â†’ Ordena de mayor a menor por probabilidad.
3. `[:3]` â†’ Toma los primeros 3.

```python
    st.subheader("Probabilidades (todas las clases)")
    prob_dict = {labels[i]: float(preds[i]) for i in range(len(labels))}
    st.bar_chart(prob_dict)
```

- Crea diccionario: `{"cardboard": 0.10, "glass": 0.04, ...}`.
- `st.bar_chart()` lo grafica.

#### Bloque 8: Estado inicial

```python
else:
    st.info("Sube una imagen para comenzar.")
```

Muestra mensaje azul informativo cuando no hay imagen.

---

## EjecuciÃ³n Local

### Iniciar la aplicaciÃ³n

```bash
streamlit run app.py
```

### QuÃ© esperar

1. Se abre el navegador automÃ¡ticamente en `http://localhost:8501`.
2. Sube una imagen de prueba.
3. Observa la predicciÃ³n, top-3 y grÃ¡fico.

### Problemas comunes

| Error                                    | Causa                          | SoluciÃ³n                              |
|------------------------------------------|--------------------------------|---------------------------------------|
| `FileNotFoundError: model/model.keras`   | No has entrenado el modelo     | Ejecuta `python train.py`             |
| `ModuleNotFoundError: No module named X` | Dependencia no instalada       | Ejecuta `pip install -r requirements.txt` |
| Puerto 8501 en uso                       | Otra instancia corriendo       | `streamlit run app.py --server.port 8502` |

---

## Despliegue en Streamlit Cloud

### Prerrequisitos

1. Cuenta en [GitHub](https://github.com).
2. Cuenta en [Streamlit Cloud](https://share.streamlit.io) (gratuita, vinculada a GitHub).
3. Repositorio pÃºblico (o privado con plan de pago).

### Paso 1: Preparar el repositorio

**Estructura mÃ­nima para deploy:**
```
repo/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ model/
    â”œâ”€â”€ model.keras
    â”œâ”€â”€ labels.json
    â””â”€â”€ meta.json
```

**Crear `.gitignore`:**
```gitignore
# No subir el dataset (muy pesado)
data/

# No subir entorno virtual
venv/
.venv/

# Archivos del sistema
__pycache__/
*.pyc
.DS_Store
```

### Paso 2: Subir a GitHub

```bash
git init
git add .
git commit -m "Initial commit: Recycle CNN classifier"
git branch -M main
git remote add origin https://github.com/tu-usuario/recycle-cnn.git
git push -u origin main
```

### Paso 3: Configurar Streamlit Cloud

1. Ir a [share.streamlit.io](https://share.streamlit.io).
2. Click en **"New app"**.
3. Seleccionar:
   - **Repository**: `tu-usuario/recycle-cnn`
   - **Branch**: `main`
   - **Main file path**: `app.py`
4. Click en **"Deploy"**.

### Paso 4: Esperar y probar

- El primer deploy tarda 5-10 minutos (instala dependencias).
- La URL serÃ¡: `https://tu-usuario-recycle-cnn.streamlit.app`.

### Checklist de deploy

- [ ] `requirements.txt` incluye todas las dependencias.
- [ ] `model/model.keras` estÃ¡ commiteado (mÃ¡x ~100MB).
- [ ] `model/labels.json` y `model/meta.json` estÃ¡n commiteados.
- [ ] `app.py` usa rutas relativas (`model/model.keras`, no `C:\Users\...`).
- [ ] No hay secretos hardcodeados (API keys, contraseÃ±as).

### Problemas frecuentes en deploy

| Problema                          | Causa                                    | SoluciÃ³n                              |
|-----------------------------------|------------------------------------------|---------------------------------------|
| App crash al cargar modelo        | TensorFlow muy pesado para recursos free | Usar versiÃ³n mÃ¡s ligera o tf-cpu      |
| `FileNotFoundError`               | Archivo no subido a Git                  | Verificar con `git status`            |
| Build timeout                     | Demasiadas dependencias                  | Reducir `requirements.txt`            |
| Modelo >100MB                     | Git LFS necesario                        | Usar Git LFS o comprimir modelo       |
